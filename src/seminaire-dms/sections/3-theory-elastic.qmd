## C'est quoi ElasticSearch ?

- [**ElasticSearch**]{.blue2} : logiciel pour l'indexation de données et la recherche de données.

- Utilisation en pratique avec [**Python**]{.green2} : packages *elasticsearch* et *elasticsearch-dsl*.


## Pourquoi ElasticSearch pour la recherche textuelle ?

| **Critères**                         | **Elasticsearch** | **SQL** | **Addok\***|
|:-------------------------------------|:-----------------:|:-------:|:-------:|
| **Recherche de texte avancée**       | ✅ | ❌ | ✅ |
| **Rapidité**                         | ✅ | ✅ | ⚠️ |
| **Précision**                        | ✅ | ❌ | ⚠️ |
| **Personnalisation des recherches**  | ✅ | ❌ | ❌ |
| **Facilité d'implémentation**        | ❌ | ✅ | ✅ |
| **Ressources**                       | ❌ | ✅ | ✅ |

**\*Addok**: géocodeur de la BAN.

<!-- a refaire car je ne connais pas addok, j'ai pas compris -->

<!-- | **Critères**                     | **Elasticsearch (Avantages)**                                                                                             | **SQL (Inconvénients)**                                                                                       |
|----------------------------------|----------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|
| **Recherche de texte avancée**   | Conçu pour la recherche full-text avec tolérance aux fautes d'orthographe (fuzzy search, correspondance partielle)         | Requêtes full-text limitées et moins adaptées aux variations d'orthographe                                    |
| **Performances**                 | Optimisé pour les recherches intensives sur de gros volumes de texte                                                       | Moins performant pour des recherches complexes ou de grandes quantités de données textuelles                  |
| **Personnalisation des scores**  | Permet de pondérer et personnaliser les scores des résultats pour une meilleure pertinence                                | Les options de personnalisation des scores sont limitées                                                      |
| **Scalabilité**                  | Distribution facile sur plusieurs nœuds pour gérer de grands ensembles de données                                         | Scalabilité plus complexe et généralement moins performante pour des recherches intensives                    |
| **Flexibilité des requêtes**     | Recherches avancées comme les synonymes, phonétique, et suggestions automatiques                                          | Requêtes avancées limitées, difficile à implémenter en SQL                                                    |
| **Complexité d'implémentation**  | Peut être complexe à mettre en œuvre pour des équipes non familières avec l'outil                                         | Plus simple et souvent mieux maîtrisé par les équipes                                                        |
| **Consommation des ressources**  | Consomme plus de mémoire et de CPU, en particulier pour l'indexation initiale                                             | Consommation de ressources généralement inférieure pour des requêtes simples                                  |
| **Coût de stockage**             | Peut impliquer une duplication des données (coût supplémentaire de stockage)                                              | Pas de duplication nécessaire                                                                                 | -->

## Outils pour créer un moteur ElasticSearch

- Settings.
- Mappings.
- Requêtes.

Éventuellement, savoir coder en Python/Java...

## Base de données classique

**Table 1: Exemple de base de données**  

| idVoie | nom de voie                 |
|---|:-----------------------------|
| A | du général leclerc           |
| B | du général charles de gaulle |
| C | du point du jour             |
| D | verdier                      |
| E | des cours                    |

## Recherche par token

[**Un token = un mot**]{.green2}  

Si on cherche **"88 avenue du général charles de gaulle"**, [**pour chaque nom de voie**]{.blue2}, on doit compter le nombre de :
"88", "avenue", "du", "général"...

[**C'est extrêment long**]{.red2}

## Index inversé token

**Table 2: Exemple d'index inversé**  

| token     | occurrences              |
|-----------|--------------------------|
| général   | {"A": 1, "B": 1}         |
| jour      | {"C": 1}                 |
| verdier   | {"B": 1}                 |
| ...       | ...                      |

[**On a directement le comptage de chaque token par idVoie**]{.green2}

## Score par token

[**Pour retourner la voie la plus pertinente**]{.blue2}, on construit un score pour chaque voie :
$$
\sum_{\text{∀t} \in \text{T}} occurence_t
$$

t = token  
T = ensemble des tokens de l'adresse recherchée  

## Recherche par n-grams de caractères

[**Contourner les fautes d'orthographes**]{.blue2} : chaque token est découpé en sous-chaînes de n caractères consécutifs  

Exemple : découpage en 3-grams de caractères du texte "avenue verdier"  

| token    | 3-grams                 |
|----------|-------------------------|
| avenue   | ave, ven, enu, nue      |
| verdier  | ver, erd, rdi, die, ier |


## Index inversé 3-grams

**Table 3: Exemple d'index inversé 3-grams**  

| 3-grams | occurrences           |
|---------|-----------------------|
| gén     | {"A": 1, "B": 1}      |
| char    | {"B": 1}              |
| our     | {"C": 1, "E": 1}      |
| ...     | ...                   |


## Score par n-grams

Pour retourner la voie la plus pertinente, on construit un score pour chaque voie :
$$
\sum_{\text{∀ngram} \in \text{N}} occurence_{ngram}
$$

N = ensemble des n-grams de l'adresse recherchée

## Limites des n-grams

$$
\downarrow \text{taille n-grams}
\Rightarrow \text{taille index inversé} \uparrow 
\Rightarrow \text{temps de recherche} \uparrow
$$

Limitation à minimum n=3 pour notre cas

## Fuzziness

Contourner les fautes d'orthographes d'une autre façon : [**fuzziness**]{.green2}  

Pour matcher deux tokens avec une fuzziness de niveau 1 = corriger l'un des tokens :

- Ajout d'une lettre
- Suppression d'une lettre
- Remplacement d'une lettre
- Échanger deux lettres de place

## Score global 

Le score global va donc combiner la somme des matchs au niveau :

- token
- n-grams
- fuzziness

Ce score prendra en compte les [**boosts**]{.green2}, le n des n-grams et le niveau de fuzziness choisis au moment du paramétrage du moteur : ceci constitue [**la requête**]{.blue2}

## Boost

On peut donner plus ou moins d'importance aux différents matchs grâce aux [**boosts**]{.green2}  

Chaque occurence est multipliée par un facteur choisi : [**boost**]{.blue2}  

Exemple de boosts : 

| match au niveau | boost |
|----------|----|
| token    | 20 |
| fuzzi 1  | 15 |
| 3-grams  | 1  |
| 4-ngrams | 1  |
| 5-grams  | 1  |

## Retour sur le score global

$$
\sum_{\text{∀dn} \in \text{N}} \sum_{\text{∀sc} \in \text{dn}} boost_{dn}*occurence_{sc}
$$

N = ensemble des niveaux de découpage de l'adresse recherchée (niveau token, niveau fuzzi...)
dn = découpage de l'adresse recherchée selon le niveau (boost associé)
sc = sous-chaîne (un token, un 3-grams...)


## Configurer le moteur

Une fois qu'on a la théorie, il faut l'appliquer en pratique.

Le package ElasticSearch permet d'indexer les données en fournissant :
- Un dataframe des données du référentiel Gaïa
- Des settings (json) où on définit nos filtres, différents analyzers, différents tokenizers...
- Des mappings (json) où pour chaque variable, on fournit un analyzer et un tokenizer.

## Filtres

Il est possible d'appliquer des "filtres" sur l'adresse recherchée et sur les adresses du référentiel :
- lowercase
- asciifolding
- ponctuation
- séparation des nombres et lettres (ex : 14bis --> 14 bis)
- suppression des espaces supplémentaires
- [**dillatation des accronymes/prise en compte des synonymes**]{.red2}

## Faire des recherches

Une fois le moteur configuré, on peut faire des requêtes.

Exemple de requêtes :