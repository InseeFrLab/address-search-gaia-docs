## C'est quoi ElasticSearch ?

- [**ElasticSearch**]{.blue2} : logiciel pour l'indexation et la recherche de données.

- Utilisation en pratique avec [**Python**]{.green2} : packages *elasticsearch* et *elasticsearch-dsl*.


## Pourquoi ElasticSearch pour la recherche textuelle ?

| **Critères**                         | **ElasticSearch** | **SQL** | **Addok**|
|:-------------------------------------|:-----------------:|:-------:|:-------:|
| Recherche de texte avancée           | ✅ | ❌ | ✅ |
| Personnalisation des recherches      | ✅ | ❌ | ❌ |
| Rapidité                             | ✅ | ✅ | ✅ |
| Précision                            | ✅ | ❌ | ⚠️ |
| Facilité d'implémentation            | ❌ | ✅ | ✅ |
| Ressources                           | ❌ | ✅ | ✅ |
| Maintenance                          | ⚠️ | ✅ | ❌ |

::: {.notes}
Donner explication addok
::: 

<!-- a refaire car je ne connais pas addok, j'ai pas compris -->

<!-- | **Critères**                     | **Elasticsearch (Avantages)**                                                                                             | **SQL (Inconvénients)**                                                                                       |
|----------------------------------|----------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|
| **Recherche de texte avancée**   | Conçu pour la recherche full-text avec tolérance aux fautes d'orthographe (fuzzy search, correspondance partielle)         | Requêtes full-text limitées et moins adaptées aux variations d'orthographe                                    |
| **Performances**                 | Optimisé pour les recherches intensives sur de gros volumes de texte                                                       | Moins performant pour des recherches complexes ou de grandes quantités de données textuelles                  |
| **Personnalisation des scores**  | Permet de pondérer et personnaliser les scores des résultats pour une meilleure pertinence                                | Les options de personnalisation des scores sont limitées                                                      |
| **Scalabilité**                  | Distribution facile sur plusieurs nœuds pour gérer de grands ensembles de données                                         | Scalabilité plus complexe et generalement moins performante pour des recherches intensives                    |
| **Flexibilité des requêtes**     | Recherches avancées comme les synonymes, phonétique, et suggestions automatiques                                          | Requêtes avancées limitées, difficile à implémenter en SQL                                                    |
| **Complexité d'implémentation**  | Peut être complexe à mettre en œuvre pour des équipes non familières avec l'outil                                         | Plus simple et souvent mieux maîtrisé par les équipes                                                        |
| **Consommation des ressources**  | Consomme plus de mémoire et de CPU, en particulier pour l'indexation initiale                                             | Consommation de ressources generalement inférieure pour des requêtes simples                                  |
| **Coût de stockage**             | Peut impliquer une duplication des données (coût supplémentaire de stockage)                                              | Pas de duplication nécessaire                                                                                 | -->

## Outils pour moteur ElasticSearch

- [**Mappings**]{.blue2} 🏷️ : spécifier les traitements à appliquer pour chaque variable de nos données.  
  - *Variable_a → Analyzer_for_numbers*  
  - *Variable_b → Analyzer_for_address_text*  
  - *Variable_c → Analyzer_for_address_text*  
- [**Settings**]{.blue2} 🛠️ : définir les différents traitements.  
  - *Analyzer_for_numbers → Filtre_α, Filtre_β + Tokenizer_1*  
  - *Analyzer_for_address_text → Filtre_γ, Filtre_δ, Filtre_ε + Tokenizer_2*  
- [**Requêtes**]{.blue2} 🔍 : recherche sur les variables définies dans les mappings.  

::: {.notes}
Et éventuellement, savoir coder en Python/Java...
::: 

On peut commencer à créer notre moteur ⚙️.

# Étape 1 : pouvoir comparer l'adresse recherchée avec les données Gaïa.

## Filtres

- Définis dans les *settings* 🛠️.  
- Normalisent les données.  
- Pour les données du référentiel et pour les adresses recherchées.

## Filtres implémentés

- Lowercase  
- Asciifolding  
- Ponctuation  
- Séparation des nombres et lettres *(ex : 14bis → 14 bis)*  
- Suppression des espaces supplémentaires  
- [**Dillatation des accronymes/prise en compte des synonymes**]{.red2}  

# Étape 2 : définir un score.

## Base de données classique

*Exemple*  

| idVoie | nom de voie             |
|---|:-----------------------------|
| A | du general leclerc           |
| B | du general charles de gaulle |
| C | du point du jour             |
| D | verdier                      |
| E | des cours                    |

## Recherche par token

- [**Un token = un mot.**]{.green2}  
- [**Pour chaque nom de voie du référentiel**]{.blue2}, compter le nombre de tokens qui matchent 🎯 avec les tokens de l'adresse recherchée.

## Score avec tokenizer "token"

[**Pour retourner la voie la plus pertinente**]{.blue2}, on construit un score pour chaque voie :
$$
score_{voie} = \sum_{\text{∀t} \in \text{T}} {nb\_occurrence}_t
$$

t = token  
T = ensemble des tokens de l'adresse recherchée  

## Score avec tokenizer "token"

*Exemple : score avec tokenizer "token" de "88 avenue du general charles de gaulle"*

| idVoie | nom de voie             | score |
|---|:-----------------------------|-------|
| A | **du general** leclerc           | 2     |
| B | **du general charles de gaulle** | 5     |
| C | **du** point **du** jour         | 2     |
| D | verdier                          | 0     |
| E | des cours                        | 0     |

[**Dans une grande base de données, c'est extrêmement long.**]{.red2}

# Étape 3 : utiliser un index inversé. Mais qu'est ce donc ?

## Index inversé token

*Exemple*  

<!-- | idVoie | nom de voie             |                        | token     | occurrences              |
|---|:-----------------------------|                        |-----------|--------------------------|
| A | du general leclerc           |                        | general   | {"A": 1, "B": 1}         |
| B | du general charles de gaulle |        devient         | jour      | {"C": 1}                 |
| C | du point du jour             |                        | du        | {"A": 1, "B": 1, "C": 2} |
| D | verdier                      |                        | ...       | ...                      |
| E | des cours                    | -->

<div style="display: flex; justify-content: space-between;">

<table>
  <thead>
    <tr>
      <th>idVoie</th>
      <th>nom de voie</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>A</td>
      <td>du general leclerc</td>
    </tr>
    <tr>
      <td>B</td>
      <td>du general charles de gaulle</td>
    </tr>
    <tr>
      <td>C</td>
      <td>du point du jour</td>
    </tr>
    <tr>
      <td>D</td>
      <td>verdier</td>
    </tr>
    <tr>
      <td>E</td>
      <td>des cours</td>
    </tr>
  </tbody>
</table>

<!-- Ajoutez un espacement entre les deux tableaux -->
<div style="margin-left: 130px;"></div>

<table>
  <thead>
    <tr>
      <th>token</th>
      <th>occurrences</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>general</td>
      <td>{"A": 1, "B": 1}</td>
    </tr>
    <tr>
      <td>jour</td>
      <td>{"C": 1}</td>
    </tr>
    <tr>
      <td>du</td>
      <td>{"A": 1, "B": 1, "C": 2}</td>
    </tr>
    <tr>
      <td>cours</td>
      <td>{"E": 1}</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
    </tr>
  </tbody>
</table>
</div>

[**Comptage direct ⚡ des occurrences**]{.green2} de chaque token de la base par idVoie.

::: {.notes}
Faire un index inversé par token, c'est récupérer tous les tokens des noms de voie dans le référentiel et on les aplati dans une colonne.

Nous obtenons directement le comptage de chaque token par idVoie, pour tous les tokens présents dans les noms de voie du référentiel.
::: 

# Étape 4 : prendre en compte les variations textuelles.

## Recherche par n-grams de caractères

[**Contourner les fautes d'orthographes**]{.blue2} : chaque token est découpé en sous-chaînes de n caractères consécutifs.  

*Exemple de découpage en 3-grams de caractères du texte "avenue verdier" :*  
**ave, ven, enu, nue, ver, erd, rdi, die, ier**

<!-- | token    | 3-grams                 |
|----------|-------------------------|
| avenue   | ave, ven, enu, nue      |
| verdier  | ver, erd, rdi, die, ier | -->


## Index inversé 3-grams

*Exemple*  

<!-- | 3-grams | occurrences           |
|---------|-----------------------|
| gen     | {"A": 1, "B": 1}      |
| char    | {"B": 1}              |
| our     | {"C": 1, "E": 1}      |
| ...     | ...                   | -->



<div style="display: flex; justify-content: space-between;">

<table>
  <thead>
    <tr>
      <th>idVoie</th>
      <th>nom de voie</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>A</td>
      <td>du general leclerc</td>
    </tr>
    <tr>
      <td>B</td>
      <td>du general charles de gaulle</td>
    </tr>
    <tr>
      <td>C</td>
      <td>du point du jour</td>
    </tr>
    <tr>
      <td>D</td>
      <td>verdier</td>
    </tr>
    <tr>
      <td>E</td>
      <td>des cours</td>
    </tr>
  </tbody>
</table>

<!-- Ajoutez un espacement entre les deux tableaux -->
<div style="margin-left: 130px;"></div>

<table>
  <thead>
    <tr>
      <th>3-gram</th>
      <th>occurrences</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>gen</td>
      <td>{"A": 1, "B": 1}</td>
    </tr>
    <tr>
      <td>char</td>
      <td>{"B": 1}</td>
    </tr>
    <tr>
      <td>our</td>
      <td>{"C": 1, "E": 1}</td>
    </tr>
    <tr>
      <td>oin</td>
      <td>{"C": 1}</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
    </tr>
  </tbody>
</table>
</div>

## Score avec tokenizer "n-grams"

Score pour chaque voie :
$$
score_{voie} = \sum_{\text{∀ngram} \in \text{N}} {nb\_occurrence}_{ngram}
$$

N = ensemble des n-grams de l'adresse recherchée

## Limites des n-grams

$$
\downarrow \text{taille n-grams}
\Rightarrow \text{taille index inversé} \uparrow 
\Rightarrow \text{temps de recherche} \uparrow
$$

Limitation à minimum n∈{3,4,5} pour notre cas.

## Fuzziness

Contourner les fautes d'orthographes d'une autre façon : [**fuzziness**]{.green2}.  

<br>

Pour matcher 🎯 deux tokens avec une fuzziness de niveau 1 = corriger l'un des tokens :

- Ajout d'une lettre.  
- Suppression d'une lettre.  
- Remplacement d'une lettre.  
- Échanger deux lettres de place.  

## Score global 

Le score global va donc combiner la somme des matchs 🎯 au niveau :

- token.  
- n-grams.  
- fuzziness.  

## Boost

On peut donner plus ou moins d'importance aux différents matchs 🎯.  
Chaque occurrence est multipliée par un facteur, appelé [**boost**]{.blue2}, qui dépend du niveau de match 🎯.

## Boosts dans Gaïa

| **match au niveau**| **boost** | **exemple** |
|----------|----|----------|
| token    | 20 | "verdier" avec "verdier" |
| fuzzi 1  | 15 | "verdier" avec "verdie" |
| 3-grams  | 1  | "ver" avec "ver" |
| 4-ngrams | 1  | "erdi" avec "erdi" |
| 5-grams  | 1  | "verdi" avec "verdi" |

*Exemple : recherche de "88 avenue verdier" et dans la base il y a la voie D : "verdier".*

## Retour sur le score global

$$
score_{voie} = \sum_{\text{∀n} \in \text{N}} \sum_{\text{∀sc} \in \text{n}} boost_{n}*{nb\_occurrence}_{sc}
$$

N = ensemble des niveaux *(niveau token, niveau fuzzi...)*  
n = niveau  
sc = sous-chaîne *(un token, un 3-grams...)*  

## Configurer le moteur ⚙️

Une fois qu'on a la théorie, il faut l'appliquer en pratique.  

Le package elasticsearch permet d'indexer les données en fournissant :  

- Un dataframe 📋 des données du référentiel Gaïa.  
- Des [**settings**]{.blue2} 🛠️ où on définit nos différents **analyzers**, englobant les filtres et le tokenizer.  
- Des [**mappings**]{.green2} 🏷️ où pour chaque **variable**, on fournit un analyzer.  

## Faire des recherches

Une fois le moteur configuré, on peut faire des requêtes 🔍.

*Requête pour retrouver la voie :*

- Match 🎯 token sur nom de voie avec fuzzi 1 → boost 20.  
- Match 🎯 token sur type de voie ou nom de voie → boost 15.  
- Match 🎯 3 à 5-grams sur nom de voie → boost 1.  

À chaque fois qu'une sous-chaîne valide l'une de ces conditions, le score va ⇡ en fonction du boost associé.